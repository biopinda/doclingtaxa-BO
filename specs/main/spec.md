# Feature Specification: PDF Monograph to Structured JSON Conversion

**Feature Branch**: `001-espe-projeto-vai`
**Created**: 2025-10-01
**Status**: Draft
**Input**: User description: "Espe projeto vai consumir monografias da fauna e flora, em PDFs, e converter para dados estruturados em JSON, usando  docling (https://github.com/docling-project/docling)"

## Execution Flow (main)
```
1. Parse user description from Input
   ‚Üí If empty: ERROR "No feature description provided"
2. Extract key concepts from description
   ‚Üí Identify: actors, actions, data, constraints
3. For each unclear aspect:
   ‚Üí Mark with [NEEDS CLARIFICATION: specific question]
4. Fill User Scenarios & Testing section
   ‚Üí If no clear user flow: ERROR "Cannot determine user scenarios"
5. Generate Functional Requirements
   ‚Üí Each requirement must be testable
   ‚Üí Mark ambiguous requirements
6. Identify Key Entities (if data involved)
7. Run Review Checklist
   ‚Üí If any [NEEDS CLARIFICATION]: WARN "Spec has uncertainties"
   ‚Üí If implementation details found: ERROR "Remove tech details"
8. Return: SUCCESS (spec ready for planning)
```

---

## ‚ö° Quick Guidelines
- ‚úÖ Focus on WHAT users need and WHY
- ‚ùå Avoid HOW to implement (no tech stack, APIs, code structure)
- üë• Written for business stakeholders, not developers

### Section Requirements
- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn't apply, remove it entirely (don't leave as "N/A")

### For AI Generation
When creating this spec from a user prompt:
1. **Mark all ambiguities**: Use [NEEDS CLARIFICATION: specific question] for any assumption you'd need to make
2. **Don't guess**: If the prompt doesn't specify something (e.g., "login system" without auth method), mark it
3. **Think like a tester**: Every vague requirement should fail the "testable and unambiguous" checklist item
4. **Common underspecified areas**:
   - User types and permissions
   - Data retention/deletion policies
   - Performance targets and scale
   - Error handling behaviors
   - Integration requirements
   - Security/compliance needs

---

## Clarifications

### Session 2025-10-01
- Q: What core data structure should the JSON output follow for each monograph? ‚Üí A: Darwin Core (DwC) standard schema with extensions for structured descriptions
- Q: How should users provide PDF files for processing? ‚Üí A: Directory scan: Process all PDFs found in a specified directory
- Q: Where should the generated JSON files be saved? ‚Üí A: MongoDB database using existing DwC schema (schema-dwc2json-taxa-mongoDBJSON.json)
- Q: What determines if extracted data is complete enough to be considered valid? ‚Üí A: Scientific name and species description required. Core fields: Forma de Vida, Substrato, Dom√≠nios Fitogeogr√°ficos, Tipos de Vegeta√ß√£o, Distribui√ß√£o Geogr√°fica. Only species-level descriptions (exclude Family/Genus descriptions). Identification keys discarded.
- Q: What are acceptable performance expectations for processing PDFs? ‚Üí A: No specific limits: Best-effort processing, handle files of any size/quantity

### Session 2025-10-02
- Q: What MongoDB schema should be used for storing taxonomic data? ‚Üí A: Use existing Darwin Core schema (specs/main/schema-dwc2json-taxa-mongoDBJSON.json) with new attributes for structured species descriptions
- Q: How should structured species descriptions be stored? ‚Üí A: Create new field 'structuredDescription' with nested morphological, ecological, and taxonomic sections extracted from PDF monographs

---

## User Scenarios & Testing *(mandatory)*

### Primary User Story
A researcher or data analyst needs to process scientific monographs about fauna and flora that exist in PDF format. These documents contain valuable taxonomic, ecological, and biological information that needs to be extracted and transformed into a structured format (JSON) to enable data analysis, integration with databases, and automated processing.

### Acceptance Scenarios
1. **Given** a directory containing PDF monographs with fauna or flora information, **When** the user specifies the directory path for processing, **Then** the system scans the directory, identifies all PDF files, and processes each one
2. **Given** multiple PDF monographs in the specified directory, **When** the system completes processing, **Then** the structured JSON data for each successfully processed document is stored in the MongoDB database
3. **Given** a malformed or corrupted PDF file in the directory, **When** the system attempts to process it, **Then** the system reports an error with clear information about what went wrong and continues processing remaining files
4. **Given** a processed PDF stored in MongoDB, **When** the user queries the database, **Then** all key taxonomic and biological information from the source document is present in hierarchical structure

### Edge Cases
- What happens when a PDF is scanned (image-based) versus text-based?
- How does the system handle PDFs with complex tables, figures, or multi-column layouts?
- What happens when a monograph contains only Family or Genus descriptions without species-level data?
- What happens when species description is present but one or more core fields (Forma de Vida, Substrato, etc.) are missing?
- How does the system distinguish between identification keys and species descriptions for exclusion?
- What happens when scientific names are inconsistent or contain synonyms within the same document?
- What happens when processing very large PDF files that may take extended time?
- How does the system handle different PDF versions or encoding formats?
- What happens if MongoDB connection is lost during processing?
- How does the system prevent duplicate processing of the same PDF file?

## Requirements *(mandatory)*

### Functional Requirements
- **FR-001**: System MUST accept PDF files containing fauna and flora monographs as input
- **FR-002**: System MUST extract text, tables, and structured information from input PDFs
- **FR-003**: System MUST convert extracted information into valid JSON format
- **FR-004**: System MUST preserve taxonomic information organized in hierarchical structure from Kingdom through Species levels, including scientific names, classification at each rank, and taxonomic descriptions
- **FR-005**: System MUST handle both fauna and flora monographs
- **FR-006**: System MUST report processing status for each document (success, failure, warnings)
- **FR-007**: System MUST handle errors gracefully and provide meaningful error messages when PDF processing fails
- **FR-008**: Users MUST be able to specify a directory path containing PDF monographs to process
- **FR-008a**: System MUST scan the specified directory and identify all PDF files for processing
- **FR-009**: System MUST store the generated JSON data in a MongoDB database
- **FR-009a**: System MUST create a database record for each successfully processed monograph containing the hierarchical taxonomy JSON structure
- **FR-010**: System MUST validate that extracted data contains at minimum: scientific name and species description
- **FR-010a**: System MUST extract the following core fields when present: Forma de Vida (life form), Substrato (substrate), Dom√≠nios Fitogeogr√°ficos (phytogeographic domains), Tipos de Vegeta√ß√£o (vegetation types), Distribui√ß√£o Geogr√°fica (geographic distribution)
- **FR-010b**: System MUST process only species-level descriptions and exclude Family or Genus-level descriptions
- **FR-010c**: System MUST discard identification keys (chave de identifica√ß√£o) and not include them in the JSON output
- **FR-011**: System MUST process PDF files of any size on a best-effort basis without artificial file size or quantity limits
- **FR-012**: System MUST report processing progress including number of files processed, currently processing file, successes, and failures

### Key Entities *(include if feature involves data)*
- **PDF Monograph**: Input document containing scientific information about fauna or flora species, including text, tables, figures, taxonomic classifications, morphological descriptions, ecological data, and distribution information
- **Darwin Core Taxon Record**: MongoDB document following DwC standard schema containing taxonomic classification (kingdom, phylum, class, order, family, genus, scientificName), nomenclatural data (scientificNameAuthorship, bibliographicCitation), distribution data, vernacular names, references, and specimen information
- **Structured Description**: New MongoDB field extension containing parsed species descriptions with sections for morphology, ecology, phenology, distribution details, and diagnostic characteristics extracted from PDF monographs
- **Processing Job**: Represents a conversion task with status tracking (pending, processing, completed, failed), timestamps, error information, and references to source PDF path and MongoDB document ID
- **Species Profile Data**: Life form (lifeForm), habitat (substrate), phytogeographic domains, vegetation types, occurrence data, and endemism status - mapped to existing DwC fields (speciesprofile, distribution)

---

## Review & Acceptance Checklist
*GATE: Automated checks run during main() execution*

### Content Quality
- [x] No implementation details (languages, frameworks, APIs)
- [x] Focused on user value and business needs
- [x] Written for non-technical stakeholders
- [x] All mandatory sections completed

### Requirement Completeness
- [x] No [NEEDS CLARIFICATION] markers remain
- [x] Requirements are testable and unambiguous
- [x] Success criteria are measurable
- [x] Scope is clearly bounded
- [x] Dependencies and assumptions identified

---

## Execution Status
*Updated by main() during processing*

- [x] User description parsed
- [x] Key concepts extracted
- [x] Ambiguities marked
- [x] User scenarios defined
- [x] Requirements generated
- [x] Entities identified
- [x] Review checklist passed

---

## Notes

Key areas clarified:
1. ‚úÖ JSON schema structure: Darwin Core (DwC) standard with extensions for structured descriptions
2. ‚úÖ Input mechanism: Directory scanning for PDF files
3. ‚úÖ Output destination: MongoDB database using existing DwC schema (schema-dwc2json-taxa-mongoDBJSON.json)
4. ‚úÖ Validation criteria: Scientific name + species description required; core fields defined
5. ‚úÖ Performance expectations: Best-effort processing without size/quantity limits
6. ‚úÖ Schema compliance: Use existing DwC fields (distribution, speciesprofile, vernacularname) + new structuredDescription field

Remaining edge cases to address during planning:
- OCR handling for scanned PDFs
- Identification key detection and exclusion logic
- MongoDB connection failure resilience
- Duplicate processing prevention
- Mapping extracted data to DwC standard fields
